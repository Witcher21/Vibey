import { createChatStream, createChatCompletion } from './services/llm.js';
import { searchWeb } from './services/webSearch.js';
import { saveMemory, recallMemory, saveChatMessage, getChatHistory } from './services/memory.js';
import { extractFileContent } from './services/fileProcessor.js';

const SYSTEM_PROMPT = `You are **Vibey**, a highly capable AI assistant. You are friendly, concise, and helpful.

Capabilities:
â€¢ You can search the web for real-time information using the web_search tool.
â€¢ You can remember facts about the user using save_memory and recall them later with recall_memory.
â€¢ You can read and analyze files the user uploads (PDF, text, code, CSV, etc.).

Guidelines:
â€¢ Use tools proactively when the user's query would benefit from fresh information.
â€¢ When the user shares personal preferences or facts, save them to memory without being asked.
â€¢ Always provide well-formatted responses using Markdown where helpful.
â€¢ Be conversational and natural â€” not robotic.`;

/* â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
   Tool executor map
   â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ */
const toolExecutors = {
  web_search: async (args) => {
    const results = await searchWeb(args.query);
    return JSON.stringify(results);
  },

  recall_memory: async (args, userId) => {
    const result = await recallMemory(userId, args.query);
    return JSON.stringify(result);
  },

  save_memory: async (args, userId) => {
    const result = await saveMemory(userId, args.key, args.value, args.category || 'general');
    return JSON.stringify(result);
  },
};

/* â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
   Main agent handler â€” SSE streaming
   â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ */

/**
 * @param {object} options
 * @param {string} options.userId
 * @param {string} options.userMessage
 * @param {object} options.res        â€” Express response (SSE)
 * @param {object|null} options.file  â€” { buffer, mimetype, originalname }
 * @param {Array|null} options.localHistory â€” UI history for guests
 */
export async function runAgent({ userId, userMessage, res, file, localHistory }) {
  /* â”€â”€ 1. Prepare SSE headers â”€â”€ */
  res.writeHead(200, {
    'Content-Type': 'text/event-stream',
    'Cache-Control': 'no-cache',
    Connection: 'keep-alive',
    'X-Accel-Buffering': 'no',
  });

  const sendEvent = (event, data) => {
    res.write(`event: ${event}\ndata: ${JSON.stringify(data)}\n\n`);
  };

  try {
    /* â”€â”€ 2. Build message context â”€â”€ */
    let history = [];
    if (userId === 'guest' && localHistory && Array.isArray(localHistory)) {
      history = localHistory;
    } else {
      history = await getChatHistory(userId, 20);
    }

    const messages = [
      { role: 'system', content: SYSTEM_PROMPT },
      ...history.map((h) => ({ role: h.role, content: h.content })),
    ];

    /* â”€â”€ 3. Handle file upload â”€â”€ */
    let fileContext = '';
    if (file) {
      sendEvent('status', { message: 'Processing uploaded fileâ€¦' });
      const extracted = await extractFileContent(file.buffer, file.mimetype, file.originalname);
      fileContext = `\n\n---\nðŸ“Ž **Uploaded file:** \`${extracted.filename}\` (${extracted.pages} page${extracted.pages > 1 ? 's' : ''})\n\n\`\`\`\n${extracted.text.slice(0, 12000)}\n\`\`\`\n---\n`;
    }

    const fullUserMessage = fileContext
      ? `${userMessage}\n${fileContext}`
      : userMessage;

    messages.push({ role: 'user', content: fullUserMessage });

    /* â”€â”€ 4. Save user message to history â”€â”€ */
    await saveChatMessage(userId, 'user', userMessage);

    /* â”€â”€ 5. First LLM call (may invoke tools) â”€â”€ */
    sendEvent('status', { message: 'Thinkingâ€¦' });

    let stream = await createChatStream(messages);
    let assistantContent = '';
    let toolCalls = [];

    for await (const chunk of stream) {
      const delta = chunk.choices[0]?.delta;
      const finishReason = chunk.choices[0]?.finish_reason;

      // Accumulate text content
      if (delta?.content) {
        assistantContent += delta.content;
        sendEvent('token', { content: delta.content });
      }

      // Accumulate tool calls
      if (delta?.tool_calls) {
        for (const tc of delta.tool_calls) {
          if (tc.index !== undefined) {
            if (!toolCalls[tc.index]) {
              toolCalls[tc.index] = {
                id: tc.id || '',
                type: 'function',
                function: { name: '', arguments: '' },
              };
            }
            if (tc.id) toolCalls[tc.index].id = tc.id;
            if (tc.function?.name) toolCalls[tc.index].function.name += tc.function.name;
            if (tc.function?.arguments) toolCalls[tc.index].function.arguments += tc.function.arguments;
          }
        }
      }

      if (finishReason === 'tool_calls') {
        // Process tool calls
        break;
      }
    }

    /* â”€â”€ 6. Execute tool calls if any â”€â”€ */
    if (toolCalls.length > 0) {
      // Add the assistant message with tool calls
      messages.push({
        role: 'assistant',
        content: assistantContent || null,
        tool_calls: toolCalls,
      });

      for (const tc of toolCalls) {
        const fnName = tc.function.name;
        const fnArgs = JSON.parse(tc.function.arguments || '{}');

        sendEvent('status', { message: `Using ${fnName.replace('_', ' ')}â€¦` });

        const executor = toolExecutors[fnName];
        let result = '{"error": "Unknown tool"}';

        if (executor) {
          try {
            result = await executor(fnArgs, userId);
          } catch (err) {
            console.error(`[Agent] Tool ${fnName} error:`, err.message);
            result = JSON.stringify({ error: err.message });
          }
        }

        messages.push({
          role: 'tool',
          tool_call_id: tc.id,
          content: result,
        });
      }

      /* â”€â”€ 7. Second LLM call â€” synthesize final answer â”€â”€ */
      sendEvent('status', { message: 'Composing responseâ€¦' });

      const followUp = await createChatCompletion(messages);
      assistantContent = '';

      for await (const chunk of followUp) {
        const delta = chunk.choices[0]?.delta;
        if (delta?.content) {
          assistantContent += delta.content;
          sendEvent('token', { content: delta.content });
        }
      }
    }

    /* â”€â”€ 8. Save assistant response to history â”€â”€ */
    if (assistantContent) {
      await saveChatMessage(userId, 'assistant', assistantContent);
    }

    sendEvent('done', { message: 'complete' });
  } catch (err) {
    console.error('[Agent] Fatal error:', err);
    sendEvent('error', { message: err.message || 'An unexpected error occurred.' });
  } finally {
    res.end();
  }
}
